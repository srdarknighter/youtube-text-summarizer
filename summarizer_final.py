# -*- coding: utf-8 -*-
"""summarizer final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u5xPk0QcWiUwS6pScgJr3xI56SVC__Rz

Installing pre-requisties and using pegasus model trained on cnn dataset
"""

!pip install transformers[sentencepiece] datasets py7zr

!pip install nltk

import torch
device = "cuda" if torch.cuda.is_available() else "cpu"
device

from transformers import pipeline

!pip install youtube-transcript-api
from youtube_transcript_api import YouTubeTranscriptApi
video_id = input("Enter the Youtube URL: ")
video_id = video_id.split("=")[1]

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
nltk.download('stopwords')
nltk.download('punkt_tab')
stopwords = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
import nltk
nltk.download('punkt')
nltk.download('wordnet')

transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])
full_transcript = ""
for i in transcript:
  words = nltk.word_tokenize(i['text'])
  words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords]
  full_transcript += ' '.join(words) + ' '

from IPython.display import YouTubeVideo
YouTubeVideo(video_id)

# pipe = pipeline('summarization', model = 'google/pegasus-cnn_dailymail', device=0 if device=="cuda" else -1)

pipe_out = ' '
for i in range(0, len(full_transcript)//1000+1):
  pipe_out += pipe(full_transcript[i*1000:(i+1)*1000], max_length = 80)[0]['summary_text'] + ' '

pipe_out

from datasets import load_dataset
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# device = "gpu"
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"
model_ckpt = "google/pegasus-cnn_dailymail"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt)

samsum =  load_dataset("samsum")

samsum['train'][0]

dialogue_len = [len(x['dialogue'].split()) for x in samsum['train']]
summary_len = [len(x['summary'].split()) for x in samsum['train']]

import pandas as pd

data = pd.DataFrame([dialogue_len, summary_len]).T
data.columns = ['Dialogue Length', 'Summary Length']

data.hist(figsize=(15,5))

def get_feature(batch):
  encodings = tokenizer(batch['dialogue'], text_target=batch['summary'],
                        max_length=1024, truncation=True)

  encodings = {'input_ids': encodings['input_ids'],
               'attention_mask': encodings['attention_mask'],
               'labels': encodings['labels']}

  return encodings

samsum_pt = samsum.map(get_feature, batched=True)

samsum_pt

columns = ['input_ids', 'labels', 'attention_mask']
samsum_pt.set_format(type='torch', columns=columns)

from transformers import DataCollatorForSeq2Seq
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

!pip install accelerate

!pip install transformers[torch]

from transformers import TrainingArguments, Trainer

# ... (Your model, tokenizer, data_collator, and dataset loading code) ...

# 1. Check and set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# 2. Move the model to the device *before* creating the Trainer
model.to(device)  # Crucial step

training_args = TrainingArguments(
    output_dir="pegasus_samsum",
    num_train_epochs=1,  # Adjust as needed
    per_device_train_batch_size=1,  # Start small
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,  # Simulate larger batch
    warmup_steps=500,
    weight_decay=0.01,
    logging_steps=10,
    evaluation_strategy="steps",
    eval_steps=500,
    save_steps=500,  # Save frequently in Colab
    fp16=True,
    fp16_opt_level="O1",
    report_to="none",  # or "tensorboard"
    dataloader_num_workers = 0,
    learning_rate=5e-5,
    # optim="paged_adamw_8bit" #Use 8 bit optimizers to save memory.
)

trainer = Trainer(
    model=model,
    args=training_args,
    tokenizer=tokenizer,
    data_collator=data_collator,
    train_dataset=samsum_pt['train'],
    eval_dataset=samsum_pt['validation']
)


trainer.train()

trainer.save_model('pegasus_samsum_model')

!zip pegasus_samsum.zip -r pegasus_samsum_model/

!unzip pegasus_samsum.zip

from datasets import load_dataset
dataset = load_dataset("seanfu112/youtube_video_summaries")

"""Streamlit instance"""

!pip install streamlit
import streamlit as st
import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
!pip install youtube-transcript-api
from transformers import pipeline
@st.cache_resource
def load_model():
  tokenizer = AutoTokenizer.from_pretrained("google/pegasus-cnn_dailymail")
  model_path = '/content/drive/MyDrive/models/model_weights.pth'
  model = AutoModelForSeq2SeqLM.from_pretrained("google/pegasus-cnn_dailymail")
  model.load_state_dict(torch.load(model_path))
  return model, tokenizer

model, tokenizer = load_model()

st.title("Youtube Video Summarizer")


from youtube_transcript_api import YouTubeTranscriptApi
video_id = input("Enter the Youtube URL: ")
video_id = video_id.split("=")[1]
transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])

if st.button("Summarize"):
  pipe = pipeline('summarization', model = model, tokenizer = tokenizer)
  pipe_out = ' '
  for i in range(0, len(transcript)//1000+1):
    chunk = transcript[i * 1000:(i + 1) * 1000]
    summary = pipe(chunk, max_length=80, min_length=20, num_beams=4)
    pipe_out += summary + ' '
  st.write("**Summary: **",pipe_out)

!streamlit run app.py

train_data = dataset['train']

!unzip pegasus_samsum.zip
!mkdir extracted_model

from google.colab import drive
drive.mount('/content/drive')

import torch
import transformers
from transformers import AutoModelForSeq2SeqLM
model_path = '/content/drive/MyDrive/models/model_weights.pth'
model = AutoModelForSeq2SeqLM.from_pretrained("google/pegasus-cnn_dailymail")
model.load_state_dict(torch.load(model_path))

import torch
model = torch.load('pegasus_samsum_model/training_args.bin')
# Assuming 'model' is your trained PyTorch model
model_save_path = '/content/drive/MyDrive/saved_models'  # Adjust this path as necessary

# Save the model state dictionary
torch.save(model.state_dict(), model_save_path)


model = torch.load('extracted_model/pegasus_samsum_model')

import torch
import transformers
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained('pegasus_samsum_model')


model_save_path = '/content/drive/MyDrive/model_inception.hdf5'

torch.save(model.state_dict(), model_save_path)

import nltk
from transformers import AutoTokenizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
nltk.download('stopwords')
nltk.download('punkt_tab')
stopwords = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
nltk.download('punkt')
nltk.download('wordnet')
from transformers import PegasusForConditionalGeneration, PegasusTokenizer

generated_summary = []
reference_summary = []
full_transcript = " "
tokenizer = PegasusTokenizer.from_pretrained("google/pegasus-cnn_dailymail")
pipe = pipeline('summarization', model = model, tokenizer = tokenizer)
for transcript in train_data['chunks'][:10]:
  # words = nltk.word_tokenize(transcript)
  # words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords]
  # full_transcript = ' '.join(words)
  full_transcript = transcript
  # print(full_transcript)
  #pass the full_transcript to the model and get the output

  pipe_out = ' '
  for i in range(0, len(full_transcript)//1000+1):
    chunk = full_transcript[i * 1000:(i + 1) * 1000]
    summary = pipe(chunk, max_length=80, min_length=20, num_beams=4)[0]['summary_text']
    pipe_out += summary + ' '
  generated_summary.append(pipe_out.strip())
  reference_summary.append(train_data['summary'][len(generated_summary)-1])

train_data['chunks'][:10][0]

generated_summary

reference_summary

generated_summary[0]

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

!pip install rouge-score
import pandas as pd
from rouge_score import rouge_scorer
scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rogueL' ,'rougeLsum'], use_stemmer=True)

scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL' ,'rougeLsum'], use_stemmer=True)
results = []

for i in range(10):
    scores = scorer.score(reference_summary[i], generated_summary[i])
    results.append({
        'rouge1_precision': scores['rouge1'].precision,
        'rouge1_recall': scores['rouge1'].recall,
        'rouge1_fmeasure': scores['rouge1'].fmeasure,
        'rouge2_precision': scores['rouge2'].precision,
        'rouge2_recall': scores['rouge2'].recall,
        'rouge2_fmeasure': scores['rouge2'].fmeasure,
        'rougeL_precision': scores['rougeL'].precision,
        'rougeL_recall': scores['rougeL'].recall,
        'rougeL_fmeasure': scores['rougeL'].fmeasure,
        'rougeLsum_precision': scores['rougeLsum'].precision,
        'rougeLsum_recall': scores['rougeLsum'].recall,
        'rougeLsum_fmeasure': scores['rougeLsum'].fmeasure,
    })

results

"""A good ROUGE score varies by summarization task and metric. ROUGE-1 scores are excellent around 0.5, with scores above 0.5 considered good and 0.4 to 0.5 moderate. For ROUGE-2, scores above 0.4 are good, and 0.2 to 0.4 are moderate. ROUGE-L scores are good around 0.4 and low at 0.3 to 0.4.

"""

import pandas as pd
results_df = pd.DataFrame(results)
average_scores = results_df.mean()
print("Average ROUGE Scores:")
print(average_scores)